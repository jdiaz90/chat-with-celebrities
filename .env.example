#################################################
# üì¶ CONFIGURACI√ìN GENERAL DEL SERVIDOR
#################################################

# Puerto donde se ejecuta el servidor Express
PORT=7500

#################################################
# üß† API LOCAL DE MODELOS (OLLAMA)
#################################################

# Direcci√≥n base de la API Ollama (ajustar seg√∫n IP local o Docker)
# Ejemplo: http://localhost:11434 o http://192.168.1.x:11434
OLLAMA_API=http://localhost:11434

# Modelo por defecto si no hay uno asignado a la celebridad
# Debe existir en tu instancia de Ollama
DEFAULT_MODEL=mistral

#################################################
# üíª CONFIGURACI√ìN DEL FRONTEND (WEBPACK DEV SERVER)
#################################################

# Puerto donde se ejecuta el cliente (React, Vue, etc.)
WEBPACK_PORT=3000

# Direcci√≥n del servidor backend al que se redirige /stream v√≠a proxy
API_PROXY=http://localhost:7500

#################################################
# üêû DEPURACI√ìN Y LOGS
#################################################

# Activa el log completo del prompt que se env√≠a al modelo
# 1 = activado | 0 = desactivado
# ‚ö†Ô∏è Solo usar en entorno de desarrollo. No recomendado en producci√≥n.
DEBUG_PROMPTS=0
